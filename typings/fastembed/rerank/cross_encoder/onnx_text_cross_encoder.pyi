"""
This type stub file was generated by pyright.
"""

from typing import Any, Iterable, Optional, Sequence
from fastembed.common import OnnxProvider
from fastembed.rerank.cross_encoder.onnx_text_model import OnnxCrossEncoderModel, TextRerankerWorker
from fastembed.rerank.cross_encoder.text_cross_encoder_base import TextCrossEncoderBase
from fastembed.common.model_description import BaseModelDescription

supported_onnx_models: list[BaseModelDescription] = ...
class OnnxTextCrossEncoder(TextCrossEncoderBase, OnnxCrossEncoderModel):
    def __init__(self, model_name: str, cache_dir: Optional[str] = ..., threads: Optional[int] = ..., providers: Optional[Sequence[OnnxProvider]] = ..., cuda: bool = ..., device_ids: Optional[list[int]] = ..., lazy_load: bool = ..., device_id: Optional[int] = ..., specific_model_path: Optional[str] = ..., **kwargs: Any) -> None:
        """
        Args:
            model_name (str): The name of the model to use.
            cache_dir (str, optional): The path to the cache directory.
                                       Can be set using the `FASTEMBED_CACHE_PATH` env variable.
                                       Defaults to `fastembed_cache` in the system's temp directory.
            threads (int, optional): The number of threads single onnxruntime session can use. Defaults to None.
            providers (Optional[Sequence[OnnxProvider]], optional): The list of onnxruntime providers to use.
                Mutually exclusive with the `cuda` and `device_ids` arguments. Defaults to None.
            cuda (bool, optional): Whether to use cuda for inference. Mutually exclusive with `providers`
                Defaults to False.
            device_ids (Optional[list[int]], optional): The list of device ids to use for data parallel processing in
                workers. Should be used with `cuda=True`, mutually exclusive with `providers`. Defaults to None.
            lazy_load (bool, optional): Whether to load the model during class initialization or on demand.
                Should be set to True when using multiple-gpu and parallel encoding. Defaults to False.
            device_id (Optional[int], optional): The device id to use for loading the model in the worker process.
            specific_model_path (Optional[str], optional): The specific path to the onnx model dir if it should be imported from somewhere else

        Raises:
            ValueError: If the model_name is not in the format <org>/<model> e.g. Xenova/ms-marco-MiniLM-L-6-v2.
        """
        ...
    
    def load_onnx_model(self) -> None:
        ...
    
    def rerank(self, query: str, documents: Iterable[str], batch_size: int = ..., **kwargs: Any) -> Iterable[float]:
        """Reranks documents based on their relevance to a given query.

        Args:
            query (str): The query string to which document relevance is calculated.
            documents (Iterable[str]): Iterable of documents to be reranked.
            batch_size (int, optional): The number of documents processed in each batch. Higher batch sizes improve speed
                                        but require more memory. Default is 64.
        Returns:
            Iterable[float]: An iterable of relevance scores for each document.
        """
        ...
    
    def rerank_pairs(self, pairs: Iterable[tuple[str, str]], batch_size: int = ..., parallel: Optional[int] = ..., **kwargs: Any) -> Iterable[float]:
        ...
    


class TextCrossEncoderWorker(TextRerankerWorker):
    def init_embedding(self, model_name: str, cache_dir: str, **kwargs: Any) -> OnnxTextCrossEncoder:
        ...
    


