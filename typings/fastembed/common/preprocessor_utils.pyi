"""
This type stub file was generated by pyright.
"""

from typing import Any
from pathlib import Path
from tokenizers import Tokenizer
from fastembed.image.transform.operators import Compose

def load_special_tokens(model_dir: Path) -> dict[str, Any]:
    ...

def load_tokenizer(model_dir: Path) -> tuple[Tokenizer, dict[str, int]]:
    ...

def load_preprocessor(model_dir: Path) -> Compose:
    ...

