"""
This type stub file was generated by pyright.
"""

from typing import Any, Iterable, Optional, Sequence, Type, Union
from fastembed.common.types import NumpyArray, OnnxProvider
from fastembed.text.text_embedding_base import TextEmbeddingBase
from fastembed.common.model_description import ModelSource, PoolingType

class TextEmbedding(TextEmbeddingBase):
    EMBEDDINGS_REGISTRY: list[Type[TextEmbeddingBase]] = ...
    @classmethod
    def list_supported_models(cls) -> list[dict[str, Any]]:
        """Lists the supported models.

        Returns:
            list[dict[str, Any]]: A list of dictionaries containing the model information.
        """
        ...
    
    @classmethod
    def add_custom_model(cls, model: str, pooling: PoolingType, normalization: bool, sources: ModelSource, dim: int, model_file: str = ..., description: str = ..., license: str = ..., size_in_gb: float = ..., additional_files: Optional[list[str]] = ...) -> None:
        ...
    
    def __init__(self, model_name: str = ..., cache_dir: Optional[str] = ..., threads: Optional[int] = ..., providers: Optional[Sequence[OnnxProvider]] = ..., cuda: bool = ..., device_ids: Optional[list[int]] = ..., lazy_load: bool = ..., **kwargs: Any) -> None:
        ...
    
    @property
    def embedding_size(self) -> int:
        """Get the embedding size of the current model"""
        ...
    
    @classmethod
    def get_embedding_size(cls, model_name: str) -> int:
        """Get the embedding size of the passed model

        Args:
            model_name (str): The name of the model to get embedding size for.

        Returns:
            int: The size of the embedding.

        Raises:
            ValueError: If the model name is not found in the supported models.
        """
        ...
    
    def embed(self, documents: Union[str, Iterable[str]], batch_size: int = ..., parallel: Optional[int] = ..., **kwargs: Any) -> Iterable[NumpyArray]:
        """
        Encode a list of documents into list of embeddings.
        We use mean pooling with attention so that the model can handle variable-length inputs.

        Args:
            documents: Iterator of documents or single document to embed
            batch_size: Batch size for encoding -- higher values will use more memory, but be faster
            parallel:
                If > 1, data-parallel encoding will be used, recommended for offline encoding of large datasets.
                If 0, use all available cores.
                If None, don't use data-parallel processing, use default onnxruntime threading instead.

        Returns:
            List of embeddings, one per document
        """
        ...
    
    def query_embed(self, query: Union[str, Iterable[str]], **kwargs: Any) -> Iterable[NumpyArray]:
        """
        Embeds queries

        Args:
            query (Union[str, Iterable[str]]): The query to embed, or an iterable e.g. list of queries.

        Returns:
            Iterable[NumpyArray]: The embeddings.
        """
        ...
    
    def passage_embed(self, texts: Iterable[str], **kwargs: Any) -> Iterable[NumpyArray]:
        """
        Embeds a list of text passages into a list of embeddings.

        Args:
            texts (Iterable[str]): The list of texts to embed.
            **kwargs: Additional keyword argument to pass to the embed method.

        Yields:
            Iterable[SparseEmbedding]: The sparse embeddings.
        """
        ...
    


