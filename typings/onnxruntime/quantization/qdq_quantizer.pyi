"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from enum import Enum
from typing import Any
from onnx import TensorProto
from .base_quantizer import BaseQuantizer, QuantizationParams
from .calibrate import TensorData
from .quant_utils import QuantizedValue

class QDQQuantTensorType(Enum):
    ACTIVATION = ...
    WEIGHT = ...
    BIAS = ...


@dataclass
class QDQQuantParamProvider:
    input_name: str
    node_name: str
    ...


class QDQTensorQuantInfo:
    def __init__(self, tensor_type=..., quant_para_provider=..., axis=..., data_type=...) -> None:
        ...
    


@dataclass
class QDQBiasQuantInfo:
    node_name: str
    input_name: str
    weight_name: str
    beta: float
    ...


@dataclass
class QDQTensorQuantParams:
    original: QuantizationParams
    converted: QuantizationParams | None
    converted_recv_nodes: set[str] | None
    def get_for_consumer(self, consumer_node_name) -> QuantizationParams:
        ...
    


@dataclass
class QDQScaleZpInitializers:
    scale: TensorProto
    zero_point: TensorProto
    ...


@dataclass
class QDQTensorScaleZpInitializers:
    original: QDQScaleZpInitializers
    converted: QDQScaleZpInitializers | None
    converted_recv_nodes: set[str] | None
    ...


@dataclass
class QDQTensorQuantizedValue:
    original: QuantizedValue
    converted: QuantizedValue | None
    converted_recv_nodes: set[str] | None
    def get_for_consumer(self, consumer_node_name) -> QuantizedValue:
        ...
    


class QDQQuantizer(BaseQuantizer):
    def __init__(self, model, per_channel, reduce_range, weight_qType, activation_qType, tensors_range, nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, extra_options=...) -> None:
        ...
    
    def quantize_activation_tensor(self, tensor_name: str): # -> None:
        """
        Adds a tensor to the list of tensors to quantize. Called by op quantizers that
        want to quantize a tensor (i.e., "mark" a tensor for quantization).

        Args:
            tensor_name: name of the tensor to quantize
        """
        ...
    
    def quantize_output_same_as_input(self, output_name: str, input_name: str, node_name: str): # -> None:
        """
        Adds a tensor to the list of tensors to quantize. Called by op quantizers that
        want to quantize an output tensor using the same quantization parameters as one of the node's inputs.

        Ex: A Tranpose node's output will typically use the same quantization parameter initializers used at
        the Transpose node's input.

        Args:
            output_name: name of the node output to quantize so that it uses the same quantization params as an input.
            input_name: name of the node input from which the output tensor will get its quantization params.
            node_name: name of the node that consumes `input_name`.
        """
        ...
    
    def quantize_weight_tensor(self, tensor_name: str): # -> None:
        """
        Adds a tensor to the list of weight tensors to quantize. Called by op quantizers that
        want to quantize a weight (i.e., "mark" a weight for quantization).

        Args:
            tensor_name: name of the weight to quantize
        """
        ...
    
    def quantize_weight_tensor_per_channel(self, tensor_name, axis): # -> None:
        ...
    
    def quantize_bias_tensor(self, node_name, bias_name, input_name, weight_name, beta=...): # -> None:
        """
        Adds a bias tensor to the list of bias tensors to quantize. Called by op quantizers that
        want to quantize a bias with bias_zero_point = 0 and bias_scale = input_scale * weight_scale * beta.
        TODO: Explain the reasoning for using this formula.

        Args:
            node_name: name of the node that consumes the bias, input, and weight tensors.
            bias_name: name of the bias tensor to quantize.
            input_name: name of the input tensor whose scale is used to compute the bias's scale.
            weight_name: name of the weight tensor whose scale is used to compute the bias's scale.
            beta: Multiplier used to compute the bias's scale.
        """
        ...
    
    def remove_node(self, node): # -> None:
        ...
    
    def remove_nodes(self): # -> None:
        ...
    
    def quantize_model(self):
        ...
    
    def try_replacing_upstream_output(self, upstream_output_name, output_name): # -> bool:
        ...
    
    def is_tensor_quantized(self, tensor_name: str): # -> bool:
        ...
    
    def is_tensor_per_channel(self, tensor_name: str, default_axis: int, op_type: str | None = ...) -> tuple[bool, int | None]:
        """
        Checks if a given tensor is configured to be quantized per-channel. If so, also returns the channel axis.

        ORT only supports per-channel quantization on static weights (i.e., ONNX initializers). If the user did not provide
        tensor quantization overrides for this tensor, then the value of self.per_channel determines if the weight
        is to be quantized per-channel.

        Params:
            tensor_name: The name of the tensor to check.
            default_axis: The default channel axis. This method checks if the normalized axis is within bounds.
                          Can be overridden via the extra_options 'QDQOpTypePerChannelSupportToAxis'
                          and 'TensorQuantOverrides'.
            op_type: Optional, defaults to None. The operator type that is the only consumer of this weight.
                     Used to access the extra option 'QDQOpTypePerChannelSupportToAxis'.
        Returns:
            A tuple (is_per_channel, axis) in which the first element indicates whether the tensor is
            quantized per-channel and the second element is the channel axis.
            The returned axis is only None if the tensor is not per-channel or the axis is out of bounds.
        """
        ...
    
    def quantize_bias_static(self, bias_name: str, bias_info: QDQBiasQuantInfo) -> str:
        """
        Quantized the bias. Zero Point == 0 and Scale == Input_Scale * Weight_Scale
        """
        ...
    
    def calc_quant_params(self, tensor_data: TensorData, quant_overrides: dict[str, Any]) -> QuantizationParams:
        """
        Calculates quantization parameters (scale/zero-point) given a tensor's min/max range and optional
        user-provided overrides.
        """
        ...
    
    def calc_graph_quant_params(self) -> dict[str, QDQTensorQuantParams]:
        """
        Calculates quantization parameters (scale/zero-point) for all tensors in the graph using each tensor's min/max range
        and optional user-provided overrides.
        """
        ...
    


