"""
This type stub file was generated by pyright.
"""

from .base_quantizer import BaseQuantizer

class ONNXQuantizer(BaseQuantizer):
    def __init__(self, model, per_channel, reduce_range, mode, static, weight_qType, activation_qType, tensors_range, nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, extra_options=...) -> None:
        ...
    
    def quantize_subgraph(self, subgraph, graph_key):
        """
        generate submodel for the subgraph, so that we re-utilize current quantization implementation.
        quantize the submodel
        update subgraph and set it back to node
        """
        ...
    
    def quantize_node_with_sub_graph(self, node):
        """
        Check subgraph, if any, quantize it and replace it.
        return new_nodes added for quantizing subgraph
        """
        ...
    
    def has_QDQ_nodes(self): # -> bool:
        """
        Detect if model already has QuantizeLinear or DequantizeLinear.
        """
        ...
    
    def find_initializer_in_path(self, initializer_name): # -> bool:
        ...
    
    def add_new_nodes(self, nodes): # -> None:
        ...
    
    def quantize_model(self):
        ...
    
    def get_tensor_type(self, tensor_name, mandatory=...): # -> None:
        ...
    
    def is_float_tensor(self, tensor_name): # -> bool:
        ...
    
    def find_quantized_value(self, input_name): # -> None:
        ...
    
    def quantize_bias_static(self, bias_name, input_name, weight_name, beta=...):
        """
        Quantized the bias. Zero Point == 0 and Scale == Input_Scale * Weight_Scale
        """
        ...
    
    def contains_tensor(self, tensor_name): # -> bool:
        """
        only check for value info and newly generated tensor names, initializers are checked separately
        """
        ...
    
    def quantize_activation(self, node, indices, from_subgraph=...): # -> tuple[None, None, None, None] | tuple[list[Any], list[Any], list[Any], list[Any]]:
        ...
    
    def quantize_weight(self, node, indices, reduce_range=..., op_level_per_channel=..., axis=..., from_subgraph=...): # -> tuple[None, None, None, None] | tuple[list[Any], list[Any], list[Any], list[Any]]:
        ...
    
    def quantize_initializer(self, weight, qType, reduce_range=..., keep_float_weight=...): # -> tuple[Any, Any, Any]:
        """
        :param weight: TensorProto initializer
        :param qType: type to quantize to
        :param keep_float_weight: Whether to quantize the weight. In some cases, we only want to qunatize scale and zero point.
                                  If keep_float_weight is False, quantize the weight, or don't quantize the weight.
        :return: quantized weight name, zero point name, scale name
        """
        ...
    
    def quantize_weight_per_channel(self, weight_name, weight_qType, channel_axis, reduce_range=..., keep_float_weight=...): # -> tuple[Any, Any, Any]:
        ...
    
    def calculate_quantization_params(self): # -> dict[Any, Any] | None:
        ...
    


