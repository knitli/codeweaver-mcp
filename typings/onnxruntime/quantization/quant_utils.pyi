"""
This type stub file was generated by pyright.
"""

import numpy
import onnx
from enum import Enum
from pathlib import Path
from onnx import ModelProto, TensorProto

__producer__ = ...
__version__ = ...
onnx_domain = ...
ms_domain = ...
QUANT_OP_NAME = ...
QUANT_INPUT_SUFFIX = ...
DEQUANT_OP_NAME = ...
DEQUANT_OUTPUT_SUFFIX = ...
TENSOR_NAME_QUANT_SUFFIX = ...
MODEL_SIZE_THRESHOLD = ...
FLOAT8_DISTRIBUTIONS = ...
type_to_name = ...
class QuantizationMode(Enum):
    IntegerOps = ...
    QLinearOps = ...
    def __str__(self) -> str:
        ...
    
    @staticmethod
    def from_string(mode): # -> QuantizationMode:
        ...
    


class QuantizedValueType(Enum):
    Input = ...
    Initializer = ...
    def __str__(self) -> str:
        ...
    
    @staticmethod
    def from_string(v): # -> QuantizedValueType:
        ...
    


class QuantType(Enum):
    QInt8 = ...
    QUInt8 = ...
    QFLOAT8E4M3FN = ...
    QInt16 = ...
    QUInt16 = ...
    QInt4 = ...
    QUInt4 = ...
    def __str__(self) -> str:
        ...
    
    @staticmethod
    def from_string(t): # -> QuantType:
        ...
    
    @property
    def tensor_type(self):
        ...
    


class QuantFormat(Enum):
    QOperator = ...
    QDQ = ...
    def __str__(self) -> str:
        ...
    
    @staticmethod
    def from_string(format): # -> QuantFormat:
        ...
    


ONNX_TYPE_TO_NP_TYPE = ...
ONNX_INT_TYPE_RANGE = ...
ONNX_INT_TYPE_SYMMETRIC_RANGE = ...
ONNX_INT_TYPE_REDUCED_RANGE = ...
def quantize_nparray(qType, arr, scale, zero_point, low=..., high=...): # -> tuple[Any, ...]:
    ...

def compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=..., min_real_range=...): # -> list[NDArray[Any]]:
    """Calculate the scale s and zero point z for the quantization relation
    r = s(q-z), where r are the original values and q are the corresponding
    quantized values.

    r and z are calculated such that every value within [rmin,rmax] has an
    approximate representation within [qmin,qmax]. In addition, qmin <= z <=
    qmax is enforced. If the symmetric flag is set to True, the interval
    [rmin,rmax] is symmetrized to [-absmax, +absmax], where
    absmax = max(abs(rmin), abs(rmax)).

    :parameter rmin: minimum value of r
    :parameter rmax: maximum value of r
    :parameter qmin: minimum value representable by the target quantization data type
    :parameter qmax: maximum value representable by the target quantization data type
    :parameter symmetric: True if the floating-point range should be made symmetric. Defaults to False.
    :parameter min_real_range: Minimum floating-point range (i.e., rmax - rmin) to enforce. Defaults to None.
    :return: zero and scale [z, s]

    """
    ...

def compute_scale_zp_float8(element_type, std): # -> list[NDArray[Any]]:
    """Calculate the scale s for a float8 type (E4M3FN).
    The function assumes the coefficient distribution and the float 8
    distribution are similar to two gaussian laws.

    :return: zero and scale [z, s]

    More details in notebook `quantization_fp8.ipynb
    <https://github.com/microsoft/onnxruntime/blob/main/docs/python/notebooks/quantization_fp8.ipynb>`_.
    """
    ...

def compute_data_quant_params(data: numpy.ndarray, quant_type: onnx.TensorProto.DataType, symmetric: bool, reduce_range: bool = ..., min_real_range: float | None = ..., rmin_override: float | None = ..., rmax_override: float | None = ...) -> tuple[numpy.ndarray, numpy.ndarray]:
    """
    Returns the zero_point and scale for the given data.

    :param data: The data for which to compute quantization parameters.
    :param quant_type: The quantization data type.
    :param symmetric: whether symmetric quantization is used or not.
    :parameter reduce_range: True if the quantization range should be reduced. Defaults to False.
    :parameter min_real_range: Minimum floating-point range (i.e., rmax - rmin) to enforce. Defaults to None.
    :parameter rmin_override: The value of rmin to use if not None. Otherwise, uses min(data).
    :parameter rmax_override: The value of rmax to use if not None. Otherwise, uses max(data).
    :return: zero point and scale
    """
    ...

def quantize_data(data, qType, symmetric, reduce_range=..., min_real_range=..., rmin_override=..., rmax_override=...) -> tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]:
    """
    :param data: data to quantize
    :param qType: data type to quantize to.
    :param symmetric: whether symmetric quantization is used or not.
    :parameter reduce_range: True if the quantization range should be reduced. Defaults to False.
    :parameter min_real_range: Minimum floating-point range (i.e., rmax - rmin) to enforce. Defaults to None.
    :parameter rmin_override: The value of rmin to use if not None. Otherwise, uses min(data).
    :parameter rmax_override: The value of rmax to use if not None. Otherwise, uses max(data).
    :return: minimum, maximum, zero point, scale, and quantized weights

    To pack weights, we compute a linear transformation

    - when data `type == uint8` mode, from `[rmin, rmax]` -> :math:`[0, 2^{b-1}]` and
    - when data `type == int8`, from `[-m , m]` -> :math:`[-(2^{b-1}-1), 2^{b-1}-1]` where
        `m = max(abs(rmin), abs(rmax))`

    and add necessary intermediate nodes to transform quantized weight to full weight using the equation

    :math:`r = S(q-z)`, where

    - *r*: real original value
    - *q*: quantized value
    - *S*: scale
    - *z*: zero point
    """
    ...

def quantize_onnx_initializer(weight: onnx.TensorProto, quant_type: onnx.TensorProto.DataType, zero_point: numpy.ndarray, scale: numpy.ndarray, axis: int | None = ..., quant_weight_name: str | None = ...) -> onnx.TensorProto:
    """
    Returns a quantized version of the given ONNX initializer.

    :param weight: The ONNX initializer to quantize.
    :param quant_type: The final quantized data type.
    :param zero_point: The zero-point value to use for quantization.
    :param scale: The scale value to use for quantization.
    :param axis: The quantization axis if quantizing per-channel. Defaults to None.
    :param quant_weight_name: The name of the quantized initializer.
                              If not specified, the quantized name is generated.
    :return: The quantized ONNX initializer.
    """
    ...

def get_qmin_qmax_for_qType(qType, reduce_range=..., symmetric=...): # -> tuple[NDArray[unsignedinteger[_8Bit]], NDArray[unsignedinteger[_8Bit]]] | tuple[NDArray[signedinteger[_8Bit]], NDArray[signedinteger[_8Bit]]] | tuple[NDArray[unsignedinteger[_16Bit]], NDArray[unsignedinteger[_16Bit]]] | tuple[NDArray[signedinteger[_16Bit]], NDArray[signedinteger[_16Bit]]] | tuple[NDArray[Any], NDArray[Any]]:
    """
    Return qmin and qmax, the minimum and maximum value representable by the given qType
    :parameter qType: onnx.onnx_pb.TensorProto.UINT8 or onnx.onnx_pb.TensorProto.UINT8
    :return: qmin, qmax
    """
    ...

def get_qrange_for_qType(qType, reduce_range=..., symmetric=...): # -> NDArray[unsignedinteger[Any]] | NDArray[signedinteger[Any]] | NDArray[unsignedinteger[_8Bit]] | NDArray[signedinteger[_8Bit]] | NDArray[unsignedinteger[_16Bit]] | NDArray[signedinteger[_16Bit]] | NDArray[Any]:
    """
    Helper function to get the quantization range for a type.
        parameter qType: quantization type.
        return: quantization range.
    """
    ...

def normalize_axis(axis: int, rank: int) -> tuple[bool, int]:
    """
    Helper function that tries to return a normalized axis in the range [0, rank - 1].
    :parameter axis: The axis to normalize.
    :parameter rank: The tensor rank (number of dimensions).
    :return (is_valid, axis_norm)
    """
    ...

def pack_bytes_to_4bit(src_8bit: bytes) -> bytearray:
    """
    Copies a source array of 8-bit values into a destination bytearray of packed 4-bit values.
    Assumes that the source values are already in the appropriate int4 range.
    :parameter src_8bit: The 8-bit element values to pack.
    :return A bytearray with every two 8-bit src elements packed into a single byte.
    """
    ...

class QuantizedInitializer:
    """
    Represents a linearly quantized weight input from ONNX operators
    """
    def __init__(self, name, initializer, rmins, rmaxs, zero_points, scales, data=..., quantized_data=..., axis=...) -> None:
        ...
    


class QuantizedValue:
    """
    Represents a linearly quantized value (input\\output\\intializer)
    """
    def __init__(self, name, new_quantized_name, scale_name, zero_point_name, quantized_value_type, axis=..., node_type=..., node_qtype=..., scale_type=...) -> None:
        ...
    


class BiasToQuantize:
    """
    Represents a bias to be quantized
    """
    def __init__(self, bias_name, input_name, weight_name) -> None:
        ...
    


def attribute_to_kwarg(attribute): # -> dict[Any, Any]:
    """
    Convert attribute to kwarg format for use with onnx.helper.make_node.
        :parameter attribute: attribute in AttributeProto format.
        :return: attribute in {key: value} format.
    """
    ...

def find_by_name(item_name, item_list): # -> None:
    """
    Helper function to find item by name in a list.
        parameter item_name: name of the item.
        parameter item_list: list of items.
        return: item if found. None otherwise.
    """
    ...

def get_elem_index(elem_name, elem_list): # -> int:
    """
    Helper function to return index of an item in a node list
    """
    ...

def get_mul_node(inputs, output, name):
    """
    Helper function to create a Mul node.
        parameter inputs: list of input names.
        parameter output: output name.
        parameter name: name of the node.
        return: Mul node in NodeProto format.
    """
    ...

def generate_identified_filename(filename: Path, identifier: str) -> Path:
    """
    Helper function to generate a identifiable filepath by concatenating the given identifier as a suffix.
    """
    ...

def apply_plot(hist, hist_edges): # -> None:
    ...

def write_calibration_table(calibration_cache, dir=...): # -> None:
    """
    Helper function to write calibration table to files.
    """
    class MyEncoder(json.JSONEncoder):
        ...
    
    

def smooth_distribution(p, eps=...): # -> None:
    """Given a discrete distribution (may have not been normalized to 1),
    smooth it by replacing zeros with eps multiplied by a scaling factor
    and taking the corresponding amount off the non-zero values.
    Ref: http://web.engr.illinois.edu/~hanj/cs412/bk3/KL-divergence.pdf
         https://github.com//apache/incubator-mxnet/blob/master/python/mxnet/contrib/quantization.py
    """
    ...

def model_has_external_data(model_path: Path): # -> bool:
    ...

def optimize_model(model_path: Path, opt_model_path: Path): # -> None:
    """
        Generate model that applies graph optimization (constant folding, etc.)
        parameter model_path: path to the original onnx model
        parameter opt_model_path: path to the optimized onnx model
    :return: optimized onnx model
    """
    ...

def add_pre_process_metadata(model: ModelProto): # -> None:
    """Tag the model that it went through quantization pre-processing"""
    ...

def model_has_pre_process_metadata(model: ModelProto) -> bool:
    """Check the model whether it went through quantization pre-processing"""
    ...

def add_infer_metadata(model: ModelProto): # -> None:
    ...

def model_has_infer_metadata(model: ModelProto) -> bool:
    ...

def load_model_with_shape_infer(model_path: Path) -> ModelProto:
    ...

def save_and_reload_model_with_shape_infer(model: ModelProto) -> ModelProto:
    ...

def tensor_proto_to_array(initializer: TensorProto) -> numpy.ndarray:
    ...

def add_quant_suffix(tensor_name: str) -> str:
    ...

def add_quant_input_suffix(tensor_name: str) -> str:
    ...

def add_quant_output_suffix(tensor_name) -> str:
    ...

def add_dequant_suffix(tensor_name) -> str:
    ...

def add_dequant_input_suffix(tensor_name) -> str:
    ...

def add_dequant_output_suffix(tensor_name) -> str:
    ...

