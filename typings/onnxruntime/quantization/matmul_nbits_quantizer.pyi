"""
This type stub file was generated by pyright.
"""

import numpy as np
import numpy.typing as npt
from onnx.onnx_pb import GraphProto, ModelProto, NodeProto, TensorProto
from .calibrate import CalibrationDataReader
from .quant_utils import QuantFormat

logger = ...
class WeightOnlyQuantConfig:
    def __init__(self, algorithm: str, quant_format: QuantFormat, op_types_to_quantize: tuple[str, ...] | None = ..., quant_axes: tuple[tuple[str, int], ...] | None = ..., customized_weight_config: dict | None = ...) -> None:
        """This is the Base class for Weight Only blockwise quantization Configuration.

        Args:
            algorithm:
                weight only quantize algorithm name.
            quant_format: QuantFormat{QOperator, QDQ}.
                QOperator format quantizes the model with quantized operators directly.
                QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.
            op_types_to_quantize (optional):
                set of operator types to quantize. Default {MatMul}
            quant_axes (dict[str, int], optional):
                op:axis, which axis to quantize for an op. Default {MatMul: 0, Gather: 1}
            customized_weight_config:
                customized weight config for nodes if needed. It is dictionary with node name as key,
                and the value is a dict of customized config.
        """
        ...
    


class RTNWeightOnlyQuantConfig(WeightOnlyQuantConfig):
    def __init__(self, ratios=..., quant_format=..., op_types_to_quantize: tuple[str, ...] | None = ..., customized_weight_config: dict | None = ...) -> None:
        """
        This is a class for round-to-nearest (RTN) algorithm Weight Only Quant Configuration.
        RTN is the most straightforward way to quantize weight using scale maps.

        Args:
            ratios:
                percentile of clip. Defaults to {}.
            quant_format (QuantFormat{QOperator, QDQ}, optional):
                QOperator format quantizes the model with quantized operators directly.
                QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.
                Defaults to QuantFormat.QOperator.
            op_types_to_quantize (optional):
                set of operator types to quantize.
            customized_weight_config:
                customized weight config for nodes if needed. It is dictionary with node name as key,
                and the value is a dict of customized config.
        """
        ...
    


class GPTQWeightOnlyQuantConfig(WeightOnlyQuantConfig):
    def __init__(self, calibration_data_reader: CalibrationDataReader | None = ..., percdamp=..., block_size=..., actorder=..., mse=..., perchannel=..., quant_format=..., op_types_to_quantize: tuple[str, ...] | None = ...) -> None:
        """
        This is a class for GPTQ algorithm Weight Only Quant Configuration.
        GPTQ algorithm provides more accurate quantization but requires more computational resources.

        Args:
            calibration_data_reader:
                a calibration data reader. It enumerates calibration data and generates inputs for the original model.
            percdamp:
                percent of the average Hessian diagonal to use for dampening.
            block_size (int, optional):
                channel number in one block to execute a GPTQ quantization iteration.
            actorder (bool, optional):
                whether rearrange Hessian matrix considering the diag's value.
            mse (bool, optional):
                whether get scale and zero point with mse error.
            perchannel (bool, optional):
                whether quantize weight per-channel.
            quant_format (QuantFormat{QOperator, QDQ}, optional):
                QOperator format quantizes the model with quantized operators directly.
                QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.
                Defaults to QuantFormat.QOperator.
            op_types_to_quantize (optional):
                set of operator types to quantize.
        """
        ...
    


class HQQWeightOnlyQuantConfig(WeightOnlyQuantConfig):
    def __init__(self, block_size=..., bits=..., axis=..., quant_format=..., op_types_to_quantize: tuple[str, ...] | None = ..., quant_axes: tuple[tuple[str, int], ...] | None = ...) -> None:
        """
        This is a class for HQQ algorithm Weight Only Quant Configuration.
        HQQ algorithm quant weight without needing calibrate data.

        Args:
            block_size (int, optional):
                channel number in one block to execute a HQQ quantization iteration.
            bits (int, optional):
                how many bits to represent weight.
            axis (int, optional):
                0 or 1. which axis to quantize. https://arxiv.org/pdf/2309.15531.pdf
            quant_format (QuantFormat{QOperator, QDQ}, optional):
                QOperator format quantizes the model with quantized operators directly.
                QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.
                Defaults to QuantFormat.QOperator.
            op_types_to_quantize (optional):
                set of operator types to quantize.
            quant_axes (dict[str, int], optional):
                op:axis, which axis to quantize for an op. Default {MatMul: 0, Gather: 1}
        """
        ...
    


class DefaultWeightOnlyQuantConfig(WeightOnlyQuantConfig):
    def __init__(self, block_size: int = ..., is_symmetric: bool = ..., accuracy_level: int | None = ..., quant_format=..., op_types_to_quantize: tuple[str, ...] | None = ..., quant_axes: tuple[tuple[str, int], ...] | None = ..., bits: int = ...) -> None:
        """
        This is a class for weight only affine quantization configuration.

        Args:
            block_size (int, optional):
                channel number in one block to execute an affine quantization iteration.
            is_symmetric (bool, optional):
                whether quantize weight symmetrically.
            accuracy_level (int, optional):
                Accuracy level of the 4-bit quantized MatMul computation.
                Refer to the MatMulNBits contrib op's 'accuracy_level' attribute for details.
                (https://github.com/microsoft/onnxruntime/blob/main/docs/ContribOperators.md#commicrosoftmatmulnbits)
            quant_format (QuantFormat{QOperator, QDQ}, optional):
                QOperator format quantizes the model with quantized operators directly.
                QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.
                Defaults to QuantFormat.QOperator.
            op_types_to_quantize (optional):
                set of operator types to quantize.
            quant_axes (dict[str, int], optional):
                op:axis, which axis to quantize for an op. Default {MatMul: 0, Gather: 1}
            bits (int, optional):
                number of bits per element after quantization. Default 4.
        """
        ...
    


class NVAWQWeightOnlyQuantConfig(WeightOnlyQuantConfig):
    def __init__(self, tokenizer_dir, dataset_name=..., cache_dir=..., calibration_method=...) -> None:
        """
        Configuration for the nvidia_awq quantization method.

        Args:
            tokenizer_dir (str): pathof the tokenizer dir.
            dataset_name (str): Name of the dataset.
            cache_dir (str): Directory for caching.
            calibration_method (str): calib method for nvidia_awq.
        """
        ...
    
    def make_model_input(self, config, input_ids_arg, attention_mask_arg, add_past_kv_inputs, device, use_fp16, use_buffer_share, add_position_ids): # -> dict[str, Any]:
        ...
    
    def get_calib_inputs(self, dataset_name, model_name, cache_dir, calib_size, batch_size, block_size, device, use_fp16, use_buffer_share, add_past_kv_inputs, max_calib_rows_to_load, add_position_ids): # -> list[Any]:
        ...
    


def is_divisible(val1, val2):
    ...

class HQQWeightOnlyQuantizer:
    def __init__(self, config: HQQWeightOnlyQuantConfig) -> None:
        ...
    
    @staticmethod
    def optimize_weights(tensor, scale, zero, min_max: list[int], axis: int = ..., opt_params: dict | None = ..., verbose=...): # -> tuple[Any, Any]:
        ...
    
    @staticmethod
    def pack_on_row_fast_248bit(pack_tensor, ori_int_tensor, bits): # -> None:
        ...
    
    def quantize_internal(self, tensor, bits=..., channel_wise=..., group_size=..., optimize=..., round_zero=..., axis=...): # -> tuple[Any, Any, Any]:
        ...
    
    def quantize(self, node: NodeProto, graph_stack: list[GraphProto]) -> list[NodeProto]:
        """
        Target node:        QOperator node:            QDQ nodes:
        MatMul              MatMulNBits                DeQuantizeLinear -> MatMul
        Gather              GatherBlockQuantized       Gather, Gather, Gather (optional) -> DequantizeLinear
        If the node is target node with fp32 or fp16 const weight, quantize the weight to int4 and
        return the new nodes.
        If QOperator format, return the corresponding QOperator nodes.
        If QDQ format, return the corresdponging QDQ nodes.
        Gather (quantized data) + Gather (scales) + Gather (optional, zero points) -> DequantizeLinear is
        not supported yet because Gather does not support int4 data.
        """
        ...
    


def get_initializer(name, graph_path: list[GraphProto]) -> tuple[TensorProto, GraphProto]:
    ...

class DefaultWeightOnlyQuantizer:
    def __init__(self, config: DefaultWeightOnlyQuantConfig) -> None:
        ...
    
    def qbits_block_quant(self, fp32weight: npt.ArrayLike) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """4b/8b quantize fp32 weight to int4 using C++ kernels."""
        ...
    
    def quantize_matmul(self, node: NodeProto, graph_stack: list[GraphProto]) -> list[NodeProto]:
        """
        Quantize weight B of MatMul node to int4 or int8.
        Currently only support 2D constant matrix and axis 0 blockwise quantization.
        """
        ...
    
    @staticmethod
    def quant_slice_symmetric(data: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
        ...
    
    @staticmethod
    def quant_slice_asymmetric(data: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        ...
    
    @staticmethod
    def pack_int8_to_int4(data: np.ndarray) -> np.ndarray:
        """Pack int8 data to int4 and store in uint8 ndarray."""
        ...
    
    @staticmethod
    def quantize_ndarray(data: np.ndarray, quantize_axis: int, block_size: int, is_symmetric: bool) -> tuple[np.ndarray, np.ndarray, np.ndarray | None]:
        """Quantize ndarray data to int4 using numpy, return (quantized data, scales, zero points)."""
        ...
    
    def quantize_gather(self, node: NodeProto, graph_stack: list[GraphProto]) -> list[NodeProto]:
        """Quantize weight data of Gather node to int4."""
        ...
    
    def quantize(self, node: NodeProto, graph_stack: list[GraphProto]) -> list[NodeProto]:
        """
        Target node:        QOperator node:            QDQ nodes:
        MatMul              MatMulNBits                DeQuantizeLinear -> MatMul
        Gather              GatherBlockQuantized       Gather, Gather, Gather (optional) -> DequantizeLinear
        If the node is target node with fp32 or fp16 const weight, quantize the weight to int4 and
        return the new nodes.
        If QOperator format, return the corresponding QOperator nodes.
        If QDQ format, return the corresdponging QDQ nodes.
        Gather (quantized data) + Gather (scales) + Gather (optional, zero points) -> DequantizeLinear is
        not supported yet because Gather does not support int4 data.
        """
        ...
    


class NVAWQWeightOnlyQuantizer:
    def __init__(self, config: NVAWQWeightOnlyQuantConfig) -> None:
        ...
    
    def quantize_awq(self, model: ModelProto | str) -> ModelProto:
        """
        Perform nvidia_awq quantization using ModelOpt's int4 quantize function.

        Args:
            model (ModelProto): The ONNX model to quantize.

        Returns:
            ModelProto: The quantized ONNX model.
        """
        ...
    


class MatMulNBitsQuantizer:
    """
    Target node:        QOperator node:            QDQ nodes:
    MatMul              MatMulNBits                DeQuantizeLinear -> MatMul
    Gather              GatherBlockQuantized       Gather, Gather, Gather (optional) -> DequantizeLinear

    Perform 4/8 bits quantization of constant weights for target nodes.
    If algo_config.quant_format is QOperator:
      - nodes are replaced by the corresponding QOperator nodes.
      - quantized weights are stored in the contrib ops.
    If algo_config.quant_format is QDQ:
      - the quantized weight is stored in a standard onnx node. For MatMul, it is DequantizeLinear. For Gather,
        it is the three Gathers, one for quantized data, one for scales and one for optional zero points.
      - The nodes are replaced by the corresponding QDQ nodes.
      - currently Gather is not supported in QDQ because Gather does not support int4 yet.
    Note:
      - for quantized gather, the memory usage of "DequantizeLinear + Gather" is the same as the original Gather
        during runtime. Therefor it is not recommended.
      - when a node is in nodes_to_exclude, and the node configuration in algo_config.customized_weight_config will be ignored.
    """
    def __init__(self, model: ModelProto | str, block_size: int = ..., is_symmetric: bool = ..., accuracy_level: int | None = ..., nodes_to_exclude=..., nodes_to_include: list[str] | None = ..., quant_format=..., op_types_to_quantize: tuple[str, ...] | None = ..., quant_axes: tuple[tuple[str, int], ...] | None = ..., algo_config: WeightOnlyQuantConfig | None = ...) -> None:
        ...
    
    def int4_quant_algo(self): # -> None:
        """4b quantize a model with RTN or GPTQ algorithm. Please refer to
        https://github.com/intel/neural-compressor/blob/master/docs/source/quantization_weight_only.md
        for more details on weight only quantization using Intel® Neural Compressor.
        """
        ...
    
    def process(self): # -> None:
        ...
    


def ort_convert_str_to_bool(value): # -> bool:
    ...

def parse_key_value_pair(s): # -> tuple[Any, int]:
    ...

def parse_args(): # -> Namespace:
    ...

if __name__ == "__main__":
    args = ...
    input_model_path = args.input_model
    output_model_path = args.output_model
    quant_format = ...
    op_types_to_quantize = ...
    quant_axes = ...
    model = ...
    quant = ...
